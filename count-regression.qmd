---
subtitle: "Poisson regression and variations"
---

# Models for Count Outcomes

---

## Acknowledgements {.unnumbered}

This content is adapted from:

-   @dobson4e, Chapter 9
-   @vittinghoff2e, Chapter 8

---

{{< include shared-config.qmd >}}

## Introduction

::: notes
This chapter presents models for [count data](probability.qmd#sec-count-vars) outcomes.


### Adding covariates

With covariates, $\lambda$ becomes a function of the covariates $\tilde X = (X_1, \dots,X_n)$, with a $\log{}$ link function (and thus an $\exp{}$ inverse-link). That is:

$$
\begin{aligned}
\mathbb E[Y | \tilde X = \tilde x,T=t] 
&= \mu(\tilde x,t)\\
\mu(\tilde x,t) 
&= \lambda(\tilde x)\cdot t\\
\lambda(\tilde x) 
&= \exp{\eta(\tilde x)}\\
\eta(\tilde x) 
&= \tilde x'\tilde \beta = \beta_0 + \beta_1 x_1 + \dots + \beta_p x_p
\end{aligned}
$$

Therefore,
$$
\begin{aligned}
\log{\{\mathbb E[Y | \tilde X = \tilde x,T=t] \}}
&= \log{\{\mu(\tilde x)\}}\\
&=\log{\{\lambda(\tilde x) \cdot t \}}\\
&=\log{\lambda(\tilde x)} + \log{t}\\
&=\log{\exp{\eta(\tilde x)}} + \log{t}\\
&=\eta(\tilde x) + \log{t}\\
&=\tilde x'\tilde\beta + \log{t}\\
&=(\beta_0 +\beta_1 x_1+\dots + \beta_p x_p) + \log{t}\\
\end{aligned}
$$

In contrast with the $X$s, $T$ enters this expression with a $\log{}$ transformation and without a corresponding $\beta$ coefficient. 

:::{.callout-note}
Terms that enter the linear component of a model without a coefficient, such as $\log{t}$ here, are called **offsets**.
:::

### Rate ratios {.smaller}

::: notes
Differences on the log-rate scale become ratios on the rate scale, 
because
:::

$$\exp{a-b} = \frac{\exp{a}}{\exp{b}}$$

[math-preqs.qmd#cor-exp-sum]

Therefore, according to this model, 
**differences of $\delta$ in covariate $x_j$ correspond to rate ratios of $\exp{\beta_j \cdot \delta}$**.

That is, letting $\tilde X_{-j}$ denote vector $\tilde X$ with element $j$ removed: 

$$
\begin{aligned}
&{
\left\{
    \log{\mathbb E[Y |{\color{red}{X_j = a}}, \tilde X_{-j}=\tilde x_{-j},T=t]}
    \atop 
    {-\log{\mathbb E[Y |{\color{red}{X_j = b}}, \tilde X_{-j}=\tilde x_{-j},T=t]}}
    \right\}
}\\
&= 
{\left\{
\log{t} + \beta_0 + \beta_1 x_1 + ... + {\color{red}{\beta_j (a)}} + ...+\beta_p x_p 
\atop 
{-\log{t} + \beta_0 + \beta_1 x_1 + ... + {\color{red}{\beta_j (b)}} + ...+\beta_p x_p}
\right\}}\\
&= \color{red}{\beta_j(a-b)}
\end{aligned}
$$

And accordingly,

$$
\begin{aligned}
\frac
{\mathbb{E}[Y |{\color{red}{X_j = a}}, \tilde X_{-j} = \tilde x_{-j}, T = t]
}
{
\mathbb E[Y |{\color{red}{X_j = b}}, \tilde X_{-j}=\tilde x_{-j},T=t]
}
= 
\exp{{\color{red}{\beta_j(a-b)}}}
\end{aligned}
$$

## Inference for count regression models

### Confidence intervals for regression coefficients and rate ratios

As usual:

$$
\beta \in \left[\ci \right]
$$

Rate ratios: exponentiate CI endpoints

$$
\exp{\beta} \in \left[\exp{\ci} \right]
$$

### Hypothesis tests for regression coefficients

$$
t = \frac{\hat \beta - \beta_0}{\hse{\hb}}
$$

Compare $t$ or $|t|$ to the tails of the standard Gaussian distribution, according to the null hypothesis.

### Comparing nested models

log(likelihood ratio) tests, as usual.

## Prediction

$$
\begin{aligned}
\hat y 
&\eqdef \hat{\mathbb E}[Y|\tilde X= \tilde x,T=t]\\
&=\hat\mu(\tilde x, t)\\
&=\hat\lambda(\tilde x) \cdot t\\
&=\exp{\hat\eta(\tilde x)} \cdot t\\
&=\exp{\tilde x'\hat{\boldsymbol{\beta}}} \cdot t
\end{aligned}
$$

## Diagnostics

### Residuals

#### Observation residuals

$$e \eqdef y - \hat y$$

#### Pearson residuals

$$r = \frac{e}{\hse{e}} \approx \frac{e}{\sqrt{\hat y}}$$

#### Standardized Pearson residuals

$$r_p = \frac{r}{\sqrt{1-h}}$$
where $h$ is the "leverage" (which we will continue to leave undefined).

#### Deviance residuals

$$
d_k = \text{sign}(y - \hat y)\left\{\sqrt{2[\ell_{\text{full}}(y) - \ell(\hat\beta; y)]}\right\}
$$

:::{.callout-note}

$$\text{sign}(x) \eqdef \frac{x}{|x|}$$
In other words:

* $\text{sign}(x) = -1$ if $x < 0$
* $\text{sign}(x) = 0$ if $x = 0$
* $\text{sign}(x) = 1$ if $x > 0$

::::{.content-hidden}

```{r}
plot(sign,xlim = c(-1,1), xlab = "x", ylab = "sign(x)")
```

::::

:::


## Zero-inflation

### Models for zero-inflated counts

We assume a latent (unobserved) binary variable, $Z$, which we model using logistic regression:

$$P(Z=1|X=x) = \pi(x) = \expit(\gamma_0 + \gamma_1x_1 +...)$$

According to this model, if $Z=1$, then $Y$ will always be zero, regardless of $X$ and $T$:

$$P(Y=0|Z=1,X=x,T=t) = 1$$

Otherwise (if $Z=0$), $Y$ will have a Poisson distribution, conditional on $X$ and $T$, as above.

Even though we never observe $Z$, we can estimate the parameters $\gamma_0$-$\gamma_p$, via maximum likelihood:

$$
\begin{aligned}
P(Y=y|X=x,T=t) &= P(Y=y,Z=1|...) + P(Y=y,Z=0|...)
\end{aligned}
$$
(by the Law of Total Probability)

where 
$$
\begin{aligned}
P(Y=y,Z=z|...) 
&= P(Y=y|Z=z,...)P(Z=z|...)
\end{aligned}
$$

---

::: {#exr-zinf-pmf}
Expand $P(Y=0|X=x,T=t)$, $P(Y=1|X=x,T=t)$ and $P(Y=y|X=x,T=t)$ into expressions involving $P(Z=1|X=x,T=t)$ and $P(Y=y|Z=0,X=x,T=t)$.
:::

---

::: {#exr-zinf-moments}

Derive the expected value and variance of $Y$, conditional on $X$ and $T$, as functions of $P(Z=1|X=x,T=t)$ and $\mathbb E[Y|Z=0,X=x,T=t]$.
:::

## Over-dispersion

### Negative binomial models

::: notes

The Poisson distribution model **forces** the variance to equal the mean. In practice, many count distributions will have a variance substantially larger than the mean (or occasionally smaller).

When we encounter this, we can try to reduce the residual variance by adding more covariates. However, there are also alternatives to the Poisson model.

Most notably, the [negative binomial model](probability.qmd#sec-nb-dist).

:::

We can still model $\mu$ as a function of $X$ and $T$ as before, and we can combine this model with zero-inflation by using it in place of the Poisson distribution for $\P(Y=y|Z=0,X=x,T=t)$.

### Quasipoisson

An alternative to Negative binomial is the "quasipoisson" distribution. I've never used it, but it seems to be a method-of-moments type approach rather than maximum likelihood. It models the variance as $\Var{Y} = \mu\theta$, and estimates $\theta$ accordingly.

See `?quasipoisson` in R for more.
